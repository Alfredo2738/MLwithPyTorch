{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alfredo2738/MLwithPyTorch/blob/main/Transformer_Block_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Multi-Head Self-Attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim (int): The dimensionality of the input and output embeddings.\n",
        "                             It's also referred to as d_model.\n",
        "            num_heads (int): The number of attention heads.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads # Dimension of each head's query, key, value\n",
        "\n",
        "        # Linear layers for Query, Key, Value projections for all heads\n",
        "        # These will be split into multiple heads later\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Output linear layer\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Computes the scaled dot-product attention.\n",
        "        Args:\n",
        "            Q (torch.Tensor): Query tensor, shape (batch_size, num_heads, seq_len, head_dim)\n",
        "            K (torch.Tensor): Key tensor, shape (batch_size, num_heads, seq_len, head_dim)\n",
        "            V (torch.Tensor): Value tensor, shape (batch_size, num_heads, seq_len, head_dim)\n",
        "            mask (torch.Tensor, optional): Mask to be applied to attention scores.\n",
        "                                           Shape (batch_size, 1, 1, seq_len) for padding mask or\n",
        "                                           (batch_size, 1, seq_len, seq_len) for look-ahead mask.\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after attention, shape (batch_size, num_heads, seq_len, head_dim)\n",
        "            torch.Tensor: Attention weights, shape (batch_size, num_heads, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        # MatMul Q and K_transpose\n",
        "        # K.transpose(-2, -1) swaps the last two dimensions (seq_len, head_dim) -> (head_dim, seq_len)\n",
        "        # scores shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Apply mask if provided (e.g., for padding or for decoder's look-ahead)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9) # Fill with a very small number where mask is 0\n",
        "\n",
        "        # Apply softmax to get attention probabilities\n",
        "        # attention_weights shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights) # Apply dropout to attention weights\n",
        "\n",
        "        # MatMul attention_weights and V\n",
        "        # output shape: (batch_size, num_heads, seq_len_q, head_dim)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for Multi-Head Self-Attention.\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor, shape (batch_size, seq_len_q, embed_dim)\n",
        "            key (torch.Tensor): Key tensor, shape (batch_size, seq_len_k, embed_dim)\n",
        "            value (torch.Tensor): Value tensor, shape (batch_size, seq_len_v, embed_dim)\n",
        "                                  (seq_len_k and seq_len_v must be the same for self-attention)\n",
        "            mask (torch.Tensor, optional): Mask to be applied.\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor, shape (batch_size, seq_len_q, embed_dim)\n",
        "            torch.Tensor: Attention weights, shape (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # 1. Perform linear projections and split into heads\n",
        "        # Q, K, V shape: (batch_size, seq_len, embed_dim)\n",
        "        Q = self.q_linear(query) # (batch_size, seq_len_q, embed_dim)\n",
        "        K = self.k_linear(key)   # (batch_size, seq_len_k, embed_dim)\n",
        "        V = self.v_linear(value) # (batch_size, seq_len_v, embed_dim)\n",
        "\n",
        "        # Reshape Q, K, V to (batch_size, num_heads, seq_len, head_dim)\n",
        "        # .view changes the shape, .transpose reorders dimensions\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # 2. Calculate scaled dot-product attention\n",
        "        # attention_output shape: (batch_size, num_heads, seq_len_q, head_dim)\n",
        "        # attention_weights shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # 3. Concatenate heads and apply final linear layer\n",
        "        # Transpose back to (batch_size, seq_len_q, num_heads, head_dim)\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
        "        # Reshape to (batch_size, seq_len_q, embed_dim)\n",
        "        attention_output = attention_output.view(batch_size, -1, self.embed_dim)\n",
        "\n",
        "        # Apply final linear projection\n",
        "        # output shape: (batch_size, seq_len_q, embed_dim)\n",
        "        output = self.out_linear(attention_output)\n",
        "        return output, attention_weights\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Position-wise Feed-Forward Network.\n",
        "    This consists of two linear transformations with a ReLU activation in between.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, ffn_dim, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim (int): Dimensionality of the input and output. (d_model)\n",
        "            ffn_dim (int): Dimensionality of the inner-layer of the FFN. (d_ff)\n",
        "                           Often 4 * embed_dim.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ffn_dim)\n",
        "        self.linear2 = nn.Linear(ffn_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU() # Or GELU, as used in many modern transformers\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor, shape (batch_size, seq_len, embed_dim)\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor, shape (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # x -> linear1 -> relu -> dropout -> linear2\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x) # Dropout is often applied after the activation\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a single Transformer Block (Encoder Block in the original paper).\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim (int): Dimensionality of input embeddings (d_model).\n",
        "            num_heads (int): Number of attention heads.\n",
        "            ffn_dim (int): Dimensionality of the inner layer of the FFN (d_ff).\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim) # Layer Normalization 1\n",
        "        self.norm2 = nn.LayerNorm(embed_dim) # Layer Normalization 2\n",
        "        self.ffn = PositionwiseFeedForward(embed_dim, ffn_dim, dropout)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor, shape (batch_size, seq_len, embed_dim)\n",
        "            mask (torch.Tensor, optional): Mask for the self-attention layer.\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor, shape (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # 1. Multi-Head Self-Attention sub-layer\n",
        "        # For self-attention, query, key, and value are all the same (x)\n",
        "        attention_out, _ = self.attention(x, x, x, mask) # We don't need attention_weights here\n",
        "        # Residual connection and Layer Normalization\n",
        "        x = self.norm1(x + self.dropout1(attention_out))\n",
        "\n",
        "        # 2. Position-wise Feed-Forward Network sub-layer\n",
        "        ffn_out = self.ffn(x)\n",
        "        # Residual connection and Layer Normalization\n",
        "        x = self.norm2(x + self.dropout2(ffn_out))\n",
        "\n",
        "        return x\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == '__main__':\n",
        "    # Hyperparameters (typical values, can be tuned)\n",
        "    embed_dim = 512      # d_model: Dimensionality of the input embeddings\n",
        "    num_heads = 8        # Number of attention heads\n",
        "    ffn_dim = 2048       # d_ff: Inner dimension of the FFN (often 4 * embed_dim)\n",
        "    dropout_rate = 0.1   # Dropout probability\n",
        "    seq_length = 100     # Length of the input sequence (e.g., number of words)\n",
        "    batch_size = 32      # Number of sequences processed in parallel\n",
        "\n",
        "    # Create a dummy input tensor\n",
        "    # Shape: (batch_size, seq_length, embed_dim)\n",
        "    dummy_input = torch.rand(batch_size, seq_length, embed_dim)\n",
        "    print(f\"Dummy input shape: {dummy_input.shape}\")\n",
        "\n",
        "    # Instantiate the Transformer Block\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, ffn_dim, dropout_rate)\n",
        "    print(\"\\nTransformer Block architecture:\")\n",
        "    print(transformer_block)\n",
        "\n",
        "    # --- Test MultiHeadSelfAttention individually (optional) ---\n",
        "    # mha = MultiHeadSelfAttention(embed_dim, num_heads, dropout_rate)\n",
        "    # mha_output, attn_weights = mha(dummy_input, dummy_input, dummy_input)\n",
        "    # print(f\"\\nMultiHeadSelfAttention output shape: {mha_output.shape}\") # (batch_size, seq_length, embed_dim)\n",
        "    # print(f\"Attention weights shape: {attn_weights.shape}\") # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "\n",
        "    # Pass the dummy input through the Transformer Block\n",
        "    # No mask is used in this simple example, but you might need one for actual tasks\n",
        "    # (e.g., to ignore padding tokens or for decoder's look-ahead)\n",
        "    output_tensor = transformer_block(dummy_input, mask=None)\n",
        "\n",
        "    print(f\"\\nOutput tensor shape after Transformer Block: {output_tensor.shape}\")\n",
        "    # Expected output shape: (batch_size, seq_length, embed_dim)\n",
        "    # The dimensionality should remain the same throughout the block.\n",
        "\n",
        "    # --- Example of a simple mask (e.g., for padding) ---\n",
        "    # Suppose the first 2 sequences in the batch have actual length 50, and the rest are shorter.\n",
        "    # This is a simplified example. Real padding masks are more involved.\n",
        "    # Mask should be (batch_size, 1, 1, seq_length) for MHA's scaled_dot_product_attention\n",
        "    # or (batch_size, seq_length) and then expanded.\n",
        "    # A '1' indicates a position to attend to, '0' to mask out.\n",
        "    # simple_mask = torch.ones(batch_size, 1, 1, seq_length)\n",
        "    # if seq_length > 50 :\n",
        "    #     simple_mask[0, :, :, 50:] = 0 # Mask out positions after 50 for the first sequence\n",
        "    #     simple_mask[1, :, :, 70:] = 0 # Mask out positions after 70 for the second sequence\n",
        "\n",
        "    # output_with_mask = transformer_block(dummy_input, mask=simple_mask)\n",
        "    # print(f\"\\nOutput tensor shape with mask: {output_with_mask.shape}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy input shape: torch.Size([32, 100, 512])\n",
            "\n",
            "Transformer Block architecture:\n",
            "TransformerBlock(\n",
            "  (attention): MultiHeadSelfAttention(\n",
            "    (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (out_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (ffn): PositionwiseFeedForward(\n",
            "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "\n",
            "Output tensor shape after Transformer Block: torch.Size([32, 100, 512])\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "NKMEsdOLFEPy",
        "outputId": "cacaa3da-9113-4863-c3aa-23f3685cfaff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}